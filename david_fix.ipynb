{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 files belonging to 20 classes.\n",
      "Found 200 files belonging to 20 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 418ms/step - accuracy: 0.3182 - loss: 2.3578 - val_accuracy: 0.7650 - val_loss: 0.7556 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 320ms/step - accuracy: 0.9063 - loss: 0.3564 - val_accuracy: 0.8900 - val_loss: 0.4605 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 329ms/step - accuracy: 0.9510 - loss: 0.1837 - val_accuracy: 0.9100 - val_loss: 0.3112 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 335ms/step - accuracy: 0.9900 - loss: 0.0619 - val_accuracy: 0.9350 - val_loss: 0.2664 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 328ms/step - accuracy: 0.9861 - loss: 0.0493 - val_accuracy: 0.9450 - val_loss: 0.2264 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 338ms/step - accuracy: 0.9906 - loss: 0.0353 - val_accuracy: 0.9300 - val_loss: 0.2895 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 343ms/step - accuracy: 0.9926 - loss: 0.0311 - val_accuracy: 0.9300 - val_loss: 0.3060 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 434ms/step - accuracy: 0.9956 - loss: 0.0190 - val_accuracy: 0.9300 - val_loss: 0.3423 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 440ms/step - accuracy: 0.9932 - loss: 0.0235 - val_accuracy: 0.9300 - val_loss: 0.3343 - learning_rate: 2.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 425ms/step - accuracy: 0.9965 - loss: 0.0182 - val_accuracy: 0.9300 - val_loss: 0.2984 - learning_rate: 2.0000e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 266ms/step - accuracy: 0.9379 - loss: 0.2216\n",
      "Test accuracy: 0.9449999928474426\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Define paths\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "# Load datasets\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "# Retrieve class names\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "# Prefetch the data for optimal performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(image, label):\n",
    "    # Apply data augmentation\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    # image = tf.image.random_zoom(image, [0.9, 1.1], [0.9, 1.1])\n",
    "    image = tf.image.random_crop(image, size=[img_height, img_width, 3])\n",
    "    # Normalize the images to the range the pre-trained model expects\n",
    "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Create the base model from the pre-trained EfficientNetB0\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Unfreeze some layers of the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(class_names)  # Set the number of classes here\n",
    "\n",
    "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 30  # Keeping a higher number of epochs with early stopping\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "# Convert the model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TFLite and saved as 'model.tflite'\")print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TFLite and saved as 'model.tflite'\")\n",
    "\n",
    "# Convert model to .h5\n",
    "model.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 836ms/step\n",
      "Predicted class: Rawon: 400-500 kalori\n",
      "Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('model3.h5')\n",
    "\n",
    "# Define function to preprocess an image file for prediction\n",
    "def preprocess_image(file_path):\n",
    "    img = image.load_img(file_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Example usage: Predicting on a single image\n",
    "image_path = 'dataset/uji/rawon2.jpg'\n",
    "preprocessed_img = preprocess_image(image_path)\n",
    "predictions = model.predict(preprocessed_img)\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "confidence = predictions[0][predicted_class]\n",
    "\n",
    "# Retrieve class labels\n",
    "class_names_file = 'labels.txt'\n",
    "with open(class_names_file, 'r') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "# Print results\n",
    "print(f'Predicted class: {class_names[predicted_class]}')\n",
    "print(f'Confidence: {confidence:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
